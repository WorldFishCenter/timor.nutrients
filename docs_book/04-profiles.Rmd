# Timor SSF nutrient profiles {#profiles}

## Methods

In this section, we identified recurrent nutritional profiles based on [RC data][data]. We aimed to determine the most appropriate number of distinct groups, or "clusters," present in our dataset. To achieve this, we used the total within sum of square (WSS) to identify the point at which grouping additional data points together does not significantly improve the clarity of the clustering. Once we established the optimal number of clusters, we applied the [K-means](https://uc-r.github.io/kmeans_clustering#:~:text=The%20gap%20statistic%20compares%20the,simulations%20of%20the%20sampling%20process.) clustering method. This is a widely-used technique that organizes data into clusters based on similarity. In our case, we grouped fishing trips together if they showed similar levels of nutrient concentrations. By doing this, we were able to observe patterns and categorize the trips according to their nutritional profiles.

To investigate the predictability of nutritional profiles based on fishing strategies, we employed a machine learning model using the XGBoost algorithm. This algorithm is also known for its ability to prevent overfitting, a critical factor for ensuring the reliability of our predictive model. Additionally, XGBoost's feature importance tool allowed us to identify the most influential predictors on the nutritional profiles.

We used quarter, habitat, gear type, vessel type and the interaction between habitat and gear type as predictors variables. The model parameters (number of trees, tree depth, loss reduction, sample size and and early stopping) were dynamically tuned during the training phase to optimize model performance. The dataset was split into two parts: 80% for training the model and 20% for testing. This allocation ensured a comprehensive dataset for training the model while allowing for effective validation. To further enhance the model's accuracy and generalizability, we applied cross-validation on 10 folds using the training set. In the final stage, we fitted the XGBoost model to the training data and evaluated its performance using metrics such as accuracy, ROC AUC, sensitivity, and specificity. These metrics provided insights into the model's discriminatory ability between different nutritional profile outcomes. The calculation of ROC curves and AUC values offered additional evaluation of the model's effectiveness.

## Results

### Clusters

The scatter plot from the k-means clustering (Figure 5.1) showed the distribution of nutrient profiles across different clusters. The first two principal components explained a significant portion of the variance, indicating distinct groupings in nutrient profiles among the fishing trips. The clear separation of clusters in this plot suggests that the fishing trips could be effectively categorized based on their nutrient content. The bar chart (Figure 5.2) displaying nutrient adequacy across clusters indicated the number of individuals meeting the Recommended Nutrient Intake (RNI) per 1kg of catch for various nutrients. The segmentation of bars into different nutrients (calcium, iron, omega-3, protein, vitamin A, zinc) across clusters showed variation in nutritional fulfillment. This suggests that different fishing strategies, represented by different clusters, result in catches with varying nutritional values.

```{r echo=FALSE, fig.cap="Cluster analysis of nutrient profiles using k-means clustering. The scatter plot visualizes the distribution of data points in a two-dimensional space defined by the first two principal components which explain 39% and 26% of the variance. The convex hulls represent the boundaries of each cluster, providing a visual guide to the cluster density and separation.", fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
library(ggplot2)

df <-
  timor.nutrients::kobo_trips %>%
  dplyr::ungroup() %>%
  dplyr::select(-Selenium_mu) %>%
  rename_nutrients_mu() %>%
  tidyr::pivot_longer(c(zinc:vitaminA), names_to = "nutrient", values_to = "kg") %>%
  dplyr::left_join(RDI_tab, by = "nutrient") %>%
  dplyr::mutate(
    nutrients_kg_per_kg = kg / weight, # standardize nutrients for 1 kg of catch
    nutrients_g_per_kg = nutrients_kg_per_kg * 1000, # convert stand nutrients in grams
    people_rni_kg = nutrients_g_per_kg / conv_factor
  ) %>%
  dplyr::select(landing_id, reporting_region, landing_date, vessel_type, habitat, gear_type, nutrient, people_rni_kg) %>%
  tidyr::pivot_wider(names_from = "nutrient", values_from = "people_rni_kg") %>%
  dplyr::mutate(quarter = lubridate::quarter(landing_date)) %>%
  dplyr::select(landing_date, quarter, dplyr::everything()) %>%
  # dplyr::filter(landing_period > "2019-01-01") %>%
  dplyr::group_by(landing_date, quarter, vessel_type, habitat, gear_type) %>%
  dplyr::summarise(dplyr::across(is.numeric, ~ median(.x, na.rm = T))) %>%
  dplyr::ungroup() %>%
  na.omit()

# factoextra::fviz_nbclust(df[ ,5:10], kmeans, method = "wss")
set.seed(555)
k2 <- kmeans(df[, 6:11], centers = 5, nstart = 500)


factoextra::fviz_cluster(k2,
  data = df[, 6:11],
  geom = c("point"),
  shape = 19
) +
  theme_minimal() +
  scale_fill_viridis_d() +
  scale_color_viridis_d() +
  labs(title = "") +
  theme(legend.position = "bottom")
```

```{r echo=FALSE, fig.cap="Distribution of nutrient adequacy across k-means clusters. The bar chart represents the number of individuals meeting the Recommended Nutrient Intake (RNI) per 1kg of catch for each nutrient within different clusters. Each bar is segmented into six categories corresponding to the nutrients analyzed: calcium (dark purple), iron (blue), omega-3 (green), protein (teal), vitamin A (dark teal), and zinc (yellow). Clusters are labeled on the y-axis, indicating distinct groupings based on nutrient profile similarities derived from the cluster analysis. The x-axis quantifies the number of individuals who meet the RNI, highlighting the variation in nutritional fulfillment across clusters.", fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
clusterdf <-
  dplyr::tibble(
    clusters = as.character(k2$cluster),
    df
  )

clusterdf %>%
  # dplyr::select(-weight) %>%
  tidyr::pivot_longer(c(zinc:vitaminA)) %>%
  dplyr::group_by(clusters, name) %>%
  dplyr::summarise(value = median(value, na.rm = T)) %>%
  ggplot(aes(value, reorder(clusters, value), fill = name)) +
  theme_minimal() +
  geom_col() +
  scale_fill_viridis_d() +
  coord_cartesian(expand = FALSE) +
  theme(legend.position = "bottom") +
  labs(x = "N. individuals meeting RNI per 1kg of catch", y = "Cluster number", fill = "")


# generate data for ML model
clusterdf %>% 
  dplyr::mutate(habitat_gear = paste(habitat, gear_type, sep = "_")) %>% 
  dplyr::select(quarter, habitat_gear, habitat, gear_type, vessel_type, cluster = clusters) %>% 
  readr::write_rds(file = paste0(system.file("model-outputs", package = "timor.nutrients"), "/ml_data.rds"))

```

### XGBoost model

The model's predictive capacity was quantitatively assessed via receiver operating characteristic (ROC) analysis across five distinct clusters. The ROC curves (see [ML model interpretation][simple]) illustrate a differential capacity of the model to classify each cluster based on the nutritional profiles derived from various fishing strategies. Cluster 2 and 5 demonstrated superior model performance, indicated by a curve proximate to the top-left, suggesting high sensitivity and specificity. Clusters 1 and 4 showed marginally lower but comparable discrimination ability. Cluster 3 indicated a slight decrease in sensitivity and exhibited the model's lowest performance, with a curve markedly farther from the ideal top-left position. Collectively, an aggregate AUC of 0.87 signifies a strong overall ability of the model to differentiate between the clusters, albeit with varying degrees of precision. These findings underscore the model's effectiveness in predicting nutritional outcomes based on fishing strategies, with implications for tailoring nutrient-sensitive fisheries management interventions.

```{r model-settings, echo=FALSE, fig.cap="Receiver Operating Characteristic (ROC) Curves with Data Points for Cluster-Based Classification. The curves delineate the sensitivity versus 1-specificity for the five clusters derived from the XGBoost classification model. Each cluster is represented by a distinct color with data points marked, which illustrates the true positive rate against the false positive rate for each respective cluster. The closeness of each curve to the top-left corner indicates the modelâ€™s classification efficacy per cluster, with Cluster 1 and 2 showing the highest performance. The overall model demonstrates substantial predictive accuracy with a composite AUC value of 0.86.", fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
df_field <-
  readr::read_rds(paste0(system.file("model-outputs", package = "timor.nutrients"), "/ml_data.rds")) %>% 
  dplyr::mutate_all(as.factor)

# DataExplorer::plot_intro(df)
# DataExplorer::plot_bar(df)

# splitting and resampling
set.seed(234)
df_split <-
  df_field %>%
  rsample::initial_split(prop = 0.8, strata = cluster)

train <- rsample::training(df_split)
test <- rsample::testing(df_split)

# Cross validation folds from training dataset
set.seed(567)
folds <- rsample::vfold_cv(train, strata = cluster)

# pre- processing
cust_rec <-
  recipes::recipe(cluster ~ ., data = train) %>%
  #  update_role(customerID, new_role = "ID") %>%
  #  step_corr(all_numeric()) %>%
  recipes::step_corr(recipes::all_numeric(), threshold = 0.7, method = "spearman") %>%
  recipes::step_zv(recipes::all_numeric()) %>% # filter zero variance
  # recipes::step_normalize(recipes::all_numeric()) %>%
  recipes::step_other(habitat, gear_type, habitat_gear) %>%
  recipes::step_dummy(recipes::all_nominal_predictors()) %>%
  themis::step_smote(cluster)

# define model
xgb_spec <- parsnip::boost_tree(
  trees = hardhat::tune(),
  tree_depth = hardhat::tune(),
  min_n = hardhat::tune(),
  loss_reduction = hardhat::tune(), ## first three: model complexity
  sample_size = hardhat::tune(),
  mtry = hardhat::tune(), ## randomness
  learn_rate = hardhat::tune(), ## step size
  stop_iter = hardhat::tune()
  ) %>%
  parsnip::set_engine("xgboost") %>%
  parsnip::set_mode("classification")

# Passing to workflow formula and Model specification
xgb_wf <-
  workflows::workflow() %>%
  workflows::add_formula(cluster ~ .) %>%
  workflows::add_model(xgb_spec)


# tuning
xgb_grid <- dials::grid_latin_hypercube(
  dials::trees(),
  dials::tree_depth(),
  dials::min_n(),
  dials::loss_reduction(),
  dials::sample_prop(),
  dials::learn_rate(),
  dials::stop_iter(),
  dials::finalize(dials::mtry(), train),
  size = 20
)

# choose model performance metrics
members_metrics <- yardstick::metric_set(
  yardstick::accuracy,
  yardstick::roc_auc,
  yardstick::sens,
  yardstick::spec
)


#doParallel::registerDoParallel(cores = 6)
#set.seed(891)
#xgb_res <- tune::tune_grid(
#  xgb_wf,
#  resamples = folds,
#  grid = xgb_grid,
#  control = tune::control_grid(save_pred = TRUE)
#)

#readr::write_rds(xgb_res, file = paste0(system.file("model-outputs", package = #"timor.nutrients"), "/xgb_res.rds"))

# dysplay tuning parameters
# xgb_res %>%
#  tune::collect_metrics() %>%
#  dplyr::filter(.metric == "roc_auc") %>%
#  dplyr::select(mean, mtry:sample_size) %>%
#  tidyr::pivot_longer(mtry:sample_size,
#                      names_to = "parameter",
#                      values_to = "value"
#  ) %>%
#  ggplot(aes(value, mean, color = parameter)) +
#  geom_point(show.legend = FALSE) +
#  facet_wrap(~parameter, scales = "free_x")


# tune::show_best(xgb_res, "roc_auc")

# select best tune
xgb_res <- readr::read_rds(paste0(system.file("model-outputs", package = "timor.nutrients"), "/xgb_res.rds"))
best_auc <- tune::select_best(xgb_res, "roc_auc")

final_xgb <- tune::finalize_workflow(xgb_wf, best_auc)

# final_xgb %>%
#  fit(data = train) %>%
#  hardhat::extract_fit_parsnip() %>%
#  vip::vip(geom = "point")

# fit
final_rs <- tune::last_fit(final_xgb, df_split, metrics = members_metrics)

# final_rs %>%
#  tune::collect_metrics()

cmat <-
  final_rs %>%
  tune::collect_predictions() %>%
  yardstick::conf_mat(cluster, .pred_class)

# show roc curves
final_rs %>%
  tune::collect_predictions() %>%
  yardstick::roc_curve(cluster, c(.pred_1:.pred_5), event_level = "second") %>%
  ggplot(aes(1 - specificity, sensitivity, color = .level)) +
  theme_minimal() +
  geom_line() +
  geom_point(size = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "grey", linetype = "dashed")+
  coord_cartesian(expand = FALSE)+
  scale_color_viridis_d() +
  labs(color = "cluster")

# show auc value
final_rs %>%
  tune::collect_predictions() %>%
  yardstick::roc_auc(cluster, c(.pred_1:.pred_5)) %>% 
  janitor::clean_names() %>% 
  dplyr::mutate(estimate = round(estimate, 2)) %>% 
  knitr::kable()
```

```{r model-explanation, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
fit2 <-
  final_xgb %>%
  fit(data = train)


feat_names <- c("quarter", "habitat_gear", "habitat", "gear_type", "vessel_type")
simple_fit <-
  final_xgb %>%
  fit(data = train)

# Set up parallel backend
doParallel::registerDoParallel(cores = 6)

ks <- kernelshap::kernelshap(
  simple_fit,
  X = train, # Assuming random row order
  bg_X = head(train, 500), # Assuming random row order
  type = "prob", # Predictions must be numeric
  feature_names = feat_names,
  parallel = TRUE,
  verbose = TRUE
)

sha <- shapviz::shapviz(ks)
shapviz::sv_importance(sha) +
  theme_minimal()


p1 <- sv_dependence(sha, "habitat")
```

## Checks and limitations

-   The distribution of both habitat types and gear types in our data is uneven. Observations in deep water and reef environments are more common compared to other habitats, and similarly, the use of gill nets is more frequent than other types of fishing gear. We need to evaluate whether this imbalance could lead to biases or issues in our model.

-   Are we considering all the possible potential good predictors?

- These color are confusing sometimes, consider change the color palette

## Next steps

Explore the model:

-   Quantify the importance of each predictor on the model outcome

-   Assess the direction of the effect of each predictor, that is analyze which features have the most impact on driving predictions towards each cluster. [SHAP Values][simple] are a good way to address that.
